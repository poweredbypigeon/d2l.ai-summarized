{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0e05c3",
   "metadata": {},
   "source": [
    "\n",
    "# üìå Summary: Softmax Regression\n",
    "\n",
    "## üî∑ Motivation\n",
    "\n",
    "Linear regression answers ‚Äúhow much?‚Äù (continuous outputs), while **classification** answers ‚Äúwhich category?‚Äù (discrete outputs).\n",
    "\n",
    "Examples:\n",
    "- Is this email spam?\n",
    "- Is the animal a cat, chicken, or dog?\n",
    "\n",
    "Some problems allow soft assignments (probabilities for each class), others require hard decisions (one category). In multi-label classification, more than one class can apply.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Problem Setup\n",
    "\n",
    "A sample problem:\n",
    "- Inputs: $ 2 \\times 2 $ grayscale image ‚Üí 4 features: $ x_1, x_2, x_3, x_4 $\n",
    "- Outputs: 3 classes (e.g., cat, chicken, dog)\n",
    "\n",
    "**Label encoding options:**\n",
    "- **Integer class** (e.g. 1 = cat, 2 = chicken, 3 = dog) ‚Äì works only when classes are ordered.\n",
    "- **One-hot encoding** ‚Äì vector of 0s and one 1, e.g.,  \n",
    "  - cat ‚Üí $ (1, 0, 0) $  \n",
    "  - chicken ‚Üí $ (0, 1, 0) $  \n",
    "  - dog ‚Üí $ (0, 0, 1) $\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Linear Model\n",
    "\n",
    "For input vector $ \\mathbf{x} \\in \\mathbb{R}^4 $, output scores (logits) $ \\mathbf{o} \\in \\mathbb{R}^3 $ are:\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{W} \\in \\mathbb{R}^{3 \\times 4} $: weights\n",
    "- $ \\mathbf{b} \\in \\mathbb{R}^{3} $: biases\n",
    "\n",
    "Each output $ o_i $ is an affine function of inputs. This layer is fully connected.\n",
    "\n",
    "$W_{ij}$ is the weight from input feature $ j $ to output class $ i $. The bias $ b_i $ shifts the output.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Softmax Function\n",
    "\n",
    "To convert output scores $ \\mathbf{o} $ to valid probabilities $ \\hat{\\mathbf{y}} $, use:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{o_i}}{\\sum_j e^{o_j}} \\quad \\text{for each class } i\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- Probabilities are positive\n",
    "- Sum to 1\n",
    "\n",
    "Softmax is monotonic: largest $ o_i $ corresponds to highest $ \\hat{y}_i $. So:\n",
    "\n",
    "$$\n",
    "\\arg\\max_j \\hat{y}_j = \\arg\\max_j o_j\n",
    "$$\n",
    "\n",
    "Inspired by Boltzmann distributions in physics, where probabilities are proportional to $ e^{-E/kT} $.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Vectorized Computation\n",
    "\n",
    "For minibatch input $ \\mathbf{X} \\in \\mathbb{R}^{n \\times d} $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
    "\\hat{\\mathbf{Y}} &= \\text{softmax}(\\mathbf{O}) \\quad \\text{(row-wise)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Efficient and avoids loops. Libraries handle numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Loss Function: Cross-Entropy\n",
    "\n",
    "It intends to calculate the probability $P(\\mathbf{Y} | \\mathbf{X})$ of labels given inputs. The loss function is the negative log-likelihood as \n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "and you can turn products into sums by taking the logarithm. \n",
    "\n",
    "Given predictions $ \\hat{\\mathbf{y}} $ and one-hot labels $ \\mathbf{y} $, the log-likelihood is:\n",
    "\n",
    "$$\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{j=1}^{q} y_j \\log \\hat{y}_j\n",
    "$$\n",
    "\n",
    "This becomes:\n",
    "\n",
    "$$\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\log \\sum_k e^{o_k} - \\sum_j y_j o_j\n",
    "$$\n",
    "\n",
    "Loss is 0 only if predicted with 100% certainty.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Gradient of the Loss\n",
    "\n",
    "Derivative of loss with respect to logit $ o_j $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial o_j} = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "Same idea as in regression: prediction minus ground truth. Works even when labels are probabilities (not just one-hot vectors).\n",
    "\n",
    "This makes gradient descent easy to implement.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Why It‚Äôs Called Cross-Entropy\n",
    "\n",
    "Cross-entropy quantifies how many bits are needed to encode the true distribution $ \\mathbf{y} $ using the predicted distribution $ \\hat{\\mathbf{y}} $.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The central idea in information theory is to quantify the\n",
    "amount of information contained in data.\n",
    "This places a  limit on our ability to compress data.\n",
    "For a distribution $P$ its *entropy*, $H[P]$, is defined as:\n",
    "\n",
    "$$H[P] = \\sum_j - P(j) \\log P(j).$$\n",
    ":eqlabel:`eq_softmax_reg_entropy`\n",
    "\n",
    "One of the fundamental theorems of information theory states\n",
    "that in order to encode data drawn randomly from the distribution $P$,\n",
    "we need at least $H[P]$ \"nats\" to encode it :cite:`Shannon.1948`.\n",
    "If you wonder what a \"nat\" is, it is the equivalent of bit\n",
    "but when using a code with base $e$ rather than one with base 2.\n",
    "Thus, one nat is $\\frac{1}{\\log(2)} \\approx 1.44$ bit.\n",
    "\n",
    "\n",
    "You might be wondering what compression has to do with prediction. Imagine that we have a stream of data that we want to compress. If it is always easy for us to predict the next token, then this data is easy to compress. Take the extreme example where every token in the stream always takes the same value. That is a very boring data stream! And not only it is boring, but it is also easy to predict. Because the tokens are always the same, we do not have to transmit any information to communicate the contents of the stream. Easy to predict, easy to compress.\n",
    "\n",
    "However if we cannot perfectly predict every event, then we might sometimes be surprised. The **surprisal** of an event is described as $-\\log P(j)$, where $P(j)$ is the probability of the event. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677f581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy H(p, q) measures how well a probability distribution q approximates a true distribution p.\n",
    "# For discrete distributions:\n",
    "#   H(p, q) = -sum_j p(j) * log(q(j))\n",
    "# If p is one-hot (true class), this reduces to -log(q(true_class)).\n",
    "# In softmax regression, p is the true label distribution, q is the predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a54c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
